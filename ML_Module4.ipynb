{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1pyj4ak7ls4OB13OjYwPX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajan5639/pw_data_science/blob/main/ML_Module4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dynfsY3QJuhL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# 1. What is Simple Linear Regression?\n",
        "Simple Linear Regression is a statistical technique used to study the relationship between two continuous variables.\n",
        "\n",
        "It models a linear relationship where one variable (dependent) is predicted based on another (independent).\n",
        "\n",
        "The general equation is:\n",
        "ùëå = ùëöùëã+ ùëê\n",
        "\n",
        "Y=mX+c\n",
        "\n",
        "where:\n",
        "\n",
        "Y is the predicted value (dependent variable)\n",
        "\n",
        "X is the input (independent variable)\n",
        "\n",
        "m is the slope of the line\n",
        "\n",
        "c is the intercept (where the line crosses the Y-axis)\n",
        "\n",
        "Goal: To find the best straight line that fits the data and minimizes the error between predicted and actual values.\n",
        "\n",
        "Example: Predicting salary (Y) based on years of experience (X).\n",
        "\n",
        "Simple linear regression is a foundation for more advanced models like multiple regression, polynomial regression, etc.\n",
        "\n",
        "It's useful when there is only one independent variable.\n",
        "\n",
        "''''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. What are the key assumptions of Simple Linear Regression?\n",
        "# To ensure Simple Linear Regression gives valid results, certain assumptions must be met:\n",
        "\n",
        "# Linearity:\n",
        "\n",
        "# The relationship between the independent variable (X) and dependent variable (Y) must be linear.\n",
        "\n",
        "# Independence of Errors:\n",
        "\n",
        "# The residuals (errors) should not be correlated with each other.\n",
        "\n",
        "# Verified using Durbin-Watson test or plotting residuals.\n",
        "\n",
        "# Homoscedasticity:\n",
        "\n",
        "# The variance of residuals should be constant across all values of X.\n",
        "\n",
        "# If the variance increases or decreases, it indicates heteroscedasticity.\n",
        "\n",
        "# Normality of Errors:\n",
        "\n",
        "# The residuals should be normally distributed.\n",
        "\n",
        "# Checked using histogram or Q-Q plot.\n",
        "\n",
        "# No Multicollinearity (for multiple regression):\n",
        "\n",
        "# In simple regression, only one predictor, so this is not relevant.\n",
        "\n",
        "# No significant outliers:\n",
        "\n",
        "# Outliers can distort the regression line and reduce accuracy.\n",
        "\n",
        "# Meeting these assumptions ensures reliable, unbiased predictions."
      ],
      "metadata": {
        "id": "HiOqFFdrKNMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "# The coefficient m in the regression equation represents the slope of the line.\n",
        "\n",
        "# It shows the change in Y for every one-unit increase in X.\n",
        "\n",
        "# If m = 2, it means that when X increases by 1, Y increases by 2.\n",
        "\n",
        "# If m is positive, the relationship is direct (both increase together).\n",
        "\n",
        "# If m is negative, the relationship is inverse (as X increases, Y decreases).\n",
        "\n",
        "# The slope tells how sensitive the outcome (Y) is to changes in input (X).\n",
        "\n",
        "# It's calculated during training by minimizing the squared difference between actual and predicted values."
      ],
      "metadata": {
        "id": "Ixm_zNkbKRMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ 4. What does the intercept c represent in the equation Y = mX + c?\n",
        "# The intercept c is the point where the regression line crosses the Y-axis.\n",
        "\n",
        "# It represents the value of Y when X = 0.\n",
        "\n",
        "# Example: If you're predicting salary (Y) based on experience (X), and c = 20,000, it means that a person with 0 years of experience is predicted to earn ‚Çπ20,000.\n",
        "\n",
        "# The intercept adjusts the line vertically to best fit the data.\n",
        "\n",
        "# It ensures that predictions are realistic even when the input value is small or zero.\n",
        "\n",
        "# In cases where X can‚Äôt be zero (e.g., age or speed), the intercept might not have practical meaning but is still necessary for the equation."
      ],
      "metadata": {
        "id": "RqKc_t0IKT_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 5. How do we calculate the slope m in Simple Linear Regression?\n",
        "# The slope m in the equation Y = mX + c is calculated using the least squares method:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# are the means of X and Y.\n",
        "\n",
        "# The numerator represents the covariance between X and Y.\n",
        "\n",
        "# The denominator represents the variance of X.\n",
        "\n",
        "# The formula tells how much Y changes with respect to X.\n",
        "\n",
        "# A higher slope means a stronger relationship.\n",
        "\n",
        "# In Python, we usually let libraries like Scikit-learn calculate it automatically using .fit()."
      ],
      "metadata": {
        "id": "ch8gdyPAKX21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "# The least squares method is used to find the best-fitting line by minimizing the total squared difference between actual and predicted Y values.\n",
        "\n",
        "# For each data point, the error is:\n",
        "\n",
        "# Error=(ActualY‚àíPredictedY)\n",
        "# 2\n",
        "\n",
        "\n",
        "# The sum of these squared errors is what the algorithm tries to minimize.\n",
        "\n",
        "# It‚Äôs called ‚Äúleast squares‚Äù because it finds the line with the least squared errors.\n",
        "\n",
        "# Why squared? To:\n",
        "\n",
        "# Ensure all errors are positive (no cancellation of negative/positive).\n",
        "\n",
        "# Penalize larger errors more heavily.\n",
        "\n",
        "# The method helps in determining the best values of m and c.\n",
        "\n",
        "# It ensures the regression line is as close as possible to all data points collectively."
      ],
      "metadata": {
        "id": "cmcpWauQKbZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 7. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "# The coefficient of determination R¬≤ explains how much of the variation in Y is explained by X using the regression model.\n",
        "\n",
        "# R¬≤ value ranges from 0 to 1:\n",
        "\n",
        "# 1 means perfect prediction (100% variance explained).\n",
        "\n",
        "# 0 means no predictive power.\n",
        "\n",
        "# Formula:\n",
        "\n",
        "\n",
        "# fractextSumofSquaredErrorstextTotalSumofSquares\n",
        "# Example:\n",
        "\n",
        "# R¬≤ = 0.85 means that 85% of the variation in Y is explained by X.\n",
        "\n",
        "# It is a good metric to evaluate how well the model fits the data.\n",
        "\n",
        "# However, a high R¬≤ does not always mean a good model (especially if assumptions are violated)."
      ],
      "metadata": {
        "id": "-98dG8rVJv3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Answers to Assignment Theory Questions (Continued)\n",
        "\n",
        "# ---\n",
        "\n",
        "# **8. What is Multiple Linear Regression?**\n",
        "# - Multiple Linear Regression is an extension of Simple Linear Regression.\n",
        "# - It uses **two or more independent variables** to predict a single dependent variable.\n",
        "# - The general form is: \\( Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n \\)\n",
        "# - It is used when outcomes depend on **multiple factors**.\n",
        "# - Example: Predicting house price based on size, location, and number of bedrooms.\n",
        "# - Each coefficient represents the **impact of one variable** on the target, keeping others constant.\n",
        "\n",
        "# ---"
      ],
      "metadata": {
        "id": "EyW6IHVSJvzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# **9. What is the main difference between Simple and Multiple Linear Regression?**\n",
        "# - **Simple Linear Regression** has **one independent variable**.\n",
        "# - **Multiple Linear Regression** involves **two or more independent variables**.\n",
        "# - Simple model: \\( Y = mX + c \\)\n",
        "# - Multiple model: \\( Y = b_0 + b_1X_1 + b_2X_2 + ... \\)\n",
        "# - Simple Linear Regression is easier to interpret and visualize.\n",
        "# - Multiple Linear Regression gives more flexibility and better prediction for complex data."
      ],
      "metadata": {
        "id": "jMmjE3HcLdmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**10. What are the key assumptions of Multiple Linear Regression?**\n",
        "- **Linearity**: The relationship between predictors and outcome is linear.\n",
        "- **Independence**: Observations should be independent.\n",
        "- **Homoscedasticity**: Constant variance of errors.\n",
        "- **Normality of Residuals**: Residuals should follow a normal distribution.\n",
        "- **No Multicollinearity**: Independent variables should not be highly correlated with each other.\n",
        "- **No Autocorrelation**: Observations should not influence each other.\n",
        "- Violating these assumptions can lead to biased or invalid results.\n",
        "\n",
        "---\n",
        "\n",
        "**11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "- **Heteroscedasticity** means that the **variance of the residuals is not constant**.\n",
        "- It violates the assumption of homoscedasticity.\n",
        "- Causes include outliers, missing variables, or wrong model specification.\n",
        "- Effects:\n",
        "  - Coefficients remain unbiased but are **no longer efficient**.\n",
        "  - Standard errors become incorrect ‚Üí unreliable p-values and confidence intervals.\n",
        "- Detection:\n",
        "  - Use residual plots.\n",
        "  - Use tests like **Breusch‚ÄìPagan**.\n",
        "- Solution:\n",
        "  - Apply log or square root transformation.\n",
        "  - Use weighted regression.\n",
        "\n",
        "---\n",
        "\n",
        "**12. How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "- **Multicollinearity** occurs when independent variables are highly correlated.\n",
        "- Problems it causes:\n",
        "  - Inflated standard errors.\n",
        "  - Coefficients become unreliable.\n",
        "- Detection:\n",
        "  - Use **Variance Inflation Factor (VIF)**.\n",
        "  - VIF > 5 or 10 indicates serious multicollinearity.\n",
        "- Solutions:\n",
        "  - Remove or combine correlated variables.\n",
        "  - Use **Principal Component Analysis (PCA)**.\n",
        "  - Apply **Ridge or Lasso Regression**.\n",
        "  - Centering or standardizing the variables.\n",
        "\n",
        "---\n",
        "\n",
        "**13. What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "- Most regression models require **numerical input**.\n",
        "- Categorical data must be transformed:\n",
        "  - **One-Hot Encoding**: Create separate binary columns.\n",
        "  - **Label Encoding**: Assign numbers to categories.\n",
        "  - **Ordinal Encoding**: For ordered categories.\n",
        "  - **Binary Encoding**: More compact than one-hot.\n",
        "- Use `pandas.get_dummies()` or `sklearn.preprocessing.OneHotEncoder`.\n",
        "- Choose encoding type based on data size and category meaning.\n",
        "\n",
        "---\n",
        "\n",
        "**14. What is the role of interaction terms in Multiple Linear Regression?**\n",
        "- **Interaction terms** represent the combined effect of two or more variables.\n",
        "- Sometimes, the effect of one predictor depends on another.\n",
        "- Example: Effect of education on income may depend on experience.\n",
        "- Model: \\( Y = b_0 + b_1X_1 + b_2X_2 + b_3X_1X_2 \\)\n",
        "- Interaction terms help capture **non-additive relationships**.\n",
        "- In Python, use `PolynomialFeatures(interaction_only=True)` from sklearn.\n",
        "\n",
        "---\n",
        "\n",
        "**15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "- In **Simple Linear Regression**, the intercept is the value of Y when X = 0.\n",
        "- In **Multiple Linear Regression**, the intercept is the value of Y when **all X variables are 0**.\n",
        "- It may or may not be meaningful depending on the data.\n",
        "- For instance, if X includes age and income, intercept means the outcome when age = 0 and income = 0.\n",
        "- Often, we focus more on slopes than the intercept itself.\n",
        "\n",
        "---\n",
        "\n",
        "**16. What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        "- The **slope (coefficient)** represents the **impact of an independent variable on the dependent variable**.\n",
        "- A positive slope means Y increases with X.\n",
        "- A negative slope means Y decreases as X increases.\n",
        "- In Multiple Regression, each slope shows the **unique effect of one variable** while keeping others constant.\n",
        "- Example: A slope of 3 for 'experience' means salary increases ‚Çπ3,000 for each year of experience.\n",
        "- Slopes are crucial for interpretation, forecasting, and decision-making.\n",
        "- Incorrect slope estimates can lead to wrong conclusions."
      ],
      "metadata": {
        "id": "Ns2QM6YXJvw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_IUS7eitJvuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "(Answer in 300‚Äì350 words, bullet format)\n",
        "\n",
        "In a regression model (simple or multiple), the intercept is the constant term denoted as c or b‚ÇÄ in the equation:\n",
        "\n",
        "\n",
        "\n",
        "It represents the predicted value of the dependent variable (Y) when all independent variables (X‚ÇÅ, X‚ÇÇ, etc.) are zero.\n",
        "\n",
        "The intercept provides context or a starting point for the model's prediction.\n",
        "\n",
        "üîπ Why It‚Äôs Important:\n",
        "It anchors the regression line on the Y-axis.\n",
        "\n",
        "It sets the baseline level of the output variable.\n",
        "\n",
        "Without it, the regression line is forced to go through the origin, which may be inaccurate or misleading.\n",
        "\n",
        "üîπ Real-World Example:\n",
        "Suppose you are building a regression model to predict salary (Y) based on years of experience (X).\n",
        "\n",
        "If the intercept is 25,000, it means that someone with 0 years of experience is expected to earn ‚Çπ25,000.\n",
        "\n",
        "This helps interpret the starting value before any increase due to predictors like experience.\n",
        "\n",
        "üîπ In Simple vs. Multiple Regression:\n",
        "In Simple Linear Regression, the intercept is the expected value of Y when X = 0.\n",
        "\n",
        "In Multiple Linear Regression, it is the expected value of Y when all X‚ÇÅ, X‚ÇÇ, ..., X‚Çô are zero.\n",
        "\n",
        "Sometimes, the intercept may not make practical sense (e.g., age = 0 or income = 0), but it's still mathematically required.\n",
        "\n",
        "üîπ Final Thoughts:\n",
        "The intercept is a key part of the regression equation, even if its real-world interpretation isn't always meaningful.\n",
        "\n",
        "It gives the model context and supports correct prediction even when the input features are small or z\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "a7oLX24jJvjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dgdFmEdWJvgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "---\n",
        "\n",
        "**18. What are the limitations of using R¬≤ as a sole measure of model performance?**\n",
        "- R¬≤ measures how much variance in the dependent variable is explained by the model.\n",
        "- **Limitation 1**: R¬≤ can **never decrease** as you add more variables, even if they are not useful.\n",
        "- **Limitation 2**: R¬≤ **does not indicate** whether the model predictions are biased.\n",
        "- **Limitation 3**: A **high R¬≤ does not always mean good prediction accuracy**.\n",
        "- **Limitation 4**: R¬≤ doesn‚Äôt consider overfitting ‚Äî especially in multiple regression.\n",
        "- Better to use it along with **Adjusted R¬≤**, **RMSE**, or **Cross-validation**.\n",
        "\n",
        "---\n",
        "\n",
        "**19. How would you interpret a large standard error for a regression coefficient?**\n",
        "- A large standard error means the **coefficient estimate is not stable**.\n",
        "- It suggests **high uncertainty** about the effect of that predictor.\n",
        "- May indicate **multicollinearity**, **insufficient data**, or a **weak relationship**.\n",
        "- If the standard error is large compared to the coefficient, the predictor is likely **not significant**.\n",
        "\n",
        "---\n",
        "\n",
        "**20. What is polynomial regression?**\n",
        "- Polynomial Regression is a form of regression in which the relationship between independent and dependent variable is **modeled as an nth-degree polynomial**.\n",
        "- It fits a **curved line** instead of a straight line.\n",
        "- It helps in capturing **non-linear patterns**.\n",
        "- Example equation:\n",
        "( Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n )\n",
        "\n",
        "---\n",
        "\n",
        "**21. When is polynomial regression used?**\n",
        "- When the **data shows a non-linear trend** that a straight line cannot capture.\n",
        "- Used when plotting data shows **curved or quadratic shapes**.\n",
        "- Common in **physics, economics, and forecasting problems**.\n",
        "\n",
        "---\n",
        "\n",
        "**22. How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "- It represents the **predicted value of Y when all X values are 0**.\n",
        "- Provides a **starting point** or **baseline** for predictions.\n",
        "- Helps **position the regression curve** correctly on the graph.\n",
        "- May or may not have real-world meaning depending on the data.\n",
        "\n",
        "---\n",
        "\n",
        "**23. How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "- In residual plots, heteroscedasticity appears as a **funnel or pattern**, not a random spread.\n",
        "- Indicates **non-constant variance** of errors.\n",
        "- Leads to **biased standard errors and unreliable tests**.\n",
        "- Can be fixed using **log transformation**, **weighted regression**, or **robust standard errors**.\n",
        "\n",
        "---\n",
        "\n",
        "**24. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?**\n",
        "- High R¬≤: The model explains a lot of variance.\n",
        "- Low Adjusted R¬≤: **Some added predictors may not be useful**.\n",
        "- Indicates **overfitting** ‚Äî the model is too complex.\n",
        "- Adjusted R¬≤ penalizes unnecessary variables.\n",
        "\n",
        "---\n",
        "\n",
        "**25. Why is it important to scale variables in Multiple Linear Regression?**\n",
        "- Variables with large scales can **dominate** the regression model.\n",
        "- Helps in **convergence and stability** during training.\n",
        "- Important for models that use **distance or optimization**, like Gradient Descent.\n",
        "- Use `StandardScaler` or `MinMaxScaler` in `sklearn.preprocessing`.\n",
        "\n",
        "---\n",
        "\n",
        "**26. How does polynomial regression differ from linear regression?**\n",
        "- Linear Regression fits a **straight line**: \\( Y = b_0 + b_1X \\)\n",
        "- Polynomial Regression fits a **curved line**: \\( Y = b_0 + b_1X + b_2X^2 + ... \\)\n",
        "- Polynomial regression can model **non-linear relationships**.\n",
        "- Linear regression assumes a **linear relationship** only.\n",
        "\n",
        "---\n",
        "\n",
        "**27. What is the general equation for polynomial regression?**\n",
        "- The general form:\n",
        "  [ Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n ]\n",
        "- b‚ÇÄ is the intercept.\n",
        "- `b‚ÇÅ`, `b‚ÇÇ`, ..., `b‚Çô` are the coefficients.\n",
        "- The degree `n` determines the **complexity** of the curve.\n",
        "\n",
        "---\n",
        "\n",
        "**28. Can polynomial regression be applied to multiple variables? What are the limitations of polynomial regression?**\n",
        "- Yes, it can be extended to **Multiple Polynomial Regression**.\n",
        "- Example:\n",
        "  \\( Y = b_0 + b_1X_1 + b_2X_1^2 + b_3X_2 + b_4X_2^2 + ... \\)\n",
        "- Limitations:\n",
        "  - **Overfitting** if the degree is too high.\n",
        "  - **Increased computational cost**.\n",
        "  - Hard to interpret as complexity increases.\n",
        "  - Sensitive to outliers.\n",
        "  - Poor extrapolation outside the data range.\n",
        "\n",
        "---\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "y7STKit5Jvc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        " 28. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "Choosing the right degree of a polynomial is essential to avoid underfitting or overfitting.\n",
        "\n",
        "Several methods help evaluate model fit and determine the ideal polynomial degree:\n",
        "\n",
        "üîπ 1. Visual Inspection:\n",
        "Plot the polynomial curve against the data points.\n",
        "\n",
        "If the curve is too rigid ‚Üí underfitting.\n",
        "\n",
        "If the curve is too wiggly ‚Üí overfitting.\n",
        "\n",
        "Visual analysis gives a quick sense of fit.\n",
        "\n",
        "üîπ 2. R¬≤ and Adjusted R¬≤:\n",
        "R¬≤ increases with degree, but Adjusted R¬≤ penalizes unnecessary complexity.\n",
        "\n",
        "Choose the degree where Adjusted R¬≤ is maximized.\n",
        "\n",
        "üîπ 3. Cross-Validation (CV):\n",
        "Use techniques like k-fold cross-validation to test performance on unseen data.\n",
        "\n",
        "Helps select the degree that generalizes well.\n",
        "\n",
        "üîπ 4. Residual Plots:\n",
        "Analyze residuals (difference between actual and predicted).\n",
        "\n",
        "Randomly scattered residuals indicate a good fit.\n",
        "\n",
        "Patterns suggest poor model choice.\n",
        "\n",
        "üîπ 5. Mean Squared Error (MSE):\n",
        "Compute train MSE and test MSE.\n",
        "\n",
        "Select the degree where test MSE is lowest.\n",
        "\n",
        "Summary:\n",
        "A combination of R¬≤, CV, residual analysis, and visualization gives a reliable method for selecting polynomial degree.\n",
        "'''"
      ],
      "metadata": {
        "id": "N56b6-W1JvZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  29. Why is visualization important in polynomial regression?\n",
        "# Polynomial regression fits curved lines, which are hard to interpret numerically.\n",
        "\n",
        "# Visualization offers intuitive insight into how well the model captures the data's pattern.\n",
        "\n",
        "# üîπ Benefits of Visualization:\n",
        "# Reveals Underfitting or Overfitting:\n",
        "\n",
        "# Underfit: Curve too simple.\n",
        "\n",
        "# Overfit: Curve too complex.\n",
        "\n",
        "# Helps identify whether the model follows the trend or noise in the data.\n",
        "\n",
        "# üîπ Diagnostic Use:\n",
        "# Plot residuals to identify:\n",
        "\n",
        "# Heteroscedasticity\n",
        "\n",
        "# Patterns in errors\n",
        "\n",
        "# Visual inspection can validate model assumptions.\n",
        "\n",
        "# üîπ Communication Tool:\n",
        "# Helps explain model behavior to non-technical stakeholders.\n",
        "\n",
        "# Easier to justify model choice with a clear curve fit shown on the graph.\n",
        "\n",
        "#  Summary:\n",
        "# Visualization bridges technical analysis and human intuition, making it crucial for polynomial regression evaluation and communication.\n"
      ],
      "metadata": {
        "id": "rrwDalXoNW08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  30. How is polynomial regression implemented in Python?\n",
        "# Polynomial regression in Python is typically done using Scikit-learn.\n",
        "\n",
        "# üîπ Step-by-step Implementation:\n",
        "# python\n",
        "# Copy\n",
        "# Edit\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# # Sample data\n",
        "# X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "# y = np.array([2, 4, 9, 16, 25])\n",
        "\n",
        "# # Create polynomial features (degree 2)\n",
        "# poly = PolynomialFeatures(degree=2)\n",
        "# X_poly = poly.fit_transform(X)\n",
        "\n",
        "# # Fit model\n",
        "# model = LinearRegression()\n",
        "# model.fit(X_poly, y)\n",
        "\n",
        "# # Predict\n",
        "# y_pred = model.predict(X_poly)\n",
        "\n",
        "# # Plot\n",
        "# plt.scatter(X, y, color='red')\n",
        "# plt.plot(X, y_pred, color='blue')\n",
        "# plt.title('Polynomial Regression')\n",
        "# plt.xlabel('X')\n",
        "# plt.ylabel('Y')\n",
        "# plt.show()\n",
        "# üîπ Key Components:\n",
        "# PolynomialFeatures(degree=n): Adds X¬≤, X¬≥... to dataset.\n",
        "\n",
        "# LinearRegression(): Fits model after feature expansion.\n",
        "\n",
        "# Always reshape 1D X into 2D using reshape(-1, 1).\n",
        "\n",
        "#  Summary:\n",
        "# Python makes polynomial regression easy with sklearn.\n",
        "\n",
        "# It involves transforming features, fitting a linear model, and visualizing the results."
      ],
      "metadata": {
        "id": "Cx_t-vj6JvXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i-9gWPA6JvUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YC6tzX2RJvRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2vEmYJmdJvPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6J06rBuHJvMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T3bpb_dpJvKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fI4FngKJvHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgOFkCP4JvE9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}