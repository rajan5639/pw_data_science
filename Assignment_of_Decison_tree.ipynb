{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFp+PHqyKJpbhqFifnwSVj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajan5639/pw_data_science/blob/main/Assignment_of_Decison_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYs7c-przVMt"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Q1 : What is a Decision Tree, and how does it work\n",
        "\n",
        "\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. It's a tree-like model where each internal node represents a feature (or attribute), each branch represents a decision rule, and each leaf node represents an outcome (or prediction). It is called a decision tree because, similar to a tree, it starts with a root and then branches off into different solutions, just like a tree would branch off into different limbs.\n",
        "\n",
        "How does it work?\n",
        "\n",
        "Start with the root node: The root node represents the entire dataset.\n",
        "\n",
        "Select the best feature: The algorithm selects the feature that best splits the data into subsets that are as pure as possible (i.e., subsets where the majority of instances belong to the same class).\n",
        "\n",
        "Create branches: Branches are created for each possible value of the selected feature.\n",
        "\n",
        "Repeat steps 2 and 3: The process is repeated for each subset of data until a stopping criterion is met (e.g., all instances in a subset belong to the same class, or a maximum depth is reached).\n",
        "\n",
        "Assign outcomes to leaf nodes: Each leaf node is assigned an outcome based on the majority class of the instances in that subset.\n",
        "\n",
        "In simpler terms, imagine you're playing a game of 20 Questions. You start with a broad question, and based on the answer, you ask more specific questions to narrow down the possibilities. A Decision Tree works in a similar way, using features to ask questions and split the data until it reaches a prediction.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "What are impurity measures in Decision Trees\n",
        "\n",
        "\n",
        "In Decision Trees, impurity measures are used to assess the homogeneity of a node in the tree. A node is considered pure if all of its data points belong to the same class. The goal of the Decision Tree algorithm is to find splits that minimize impurity and create the purest possible nodes.\n",
        "\n",
        "Common Impurity Measures\n",
        "\n",
        "There are two main impurity measures commonly used in Decision Trees:\n",
        "\n",
        "Gini Impurity: This measure calculates the probability of misclassifying a randomly chosen element from a set. It ranges from 0 (pure node) to 1 (completely impure node). The formula for Gini impurity is:\n",
        "\n",
        "Gini Impurity = 1 - Σ(pᵢ)²\n",
        "Use code with caution\n",
        "where pᵢ is the probability of an element belonging to class i.\n",
        "\n",
        "Entropy: This measure quantifies the uncertainty or randomness in a set. It's based on information theory and ranges from 0 (pure node) to log₂(k) (completely impure node), where k is the number of classes. The formula for entropy is:\n",
        "\n",
        "Entropy = -Σ(pᵢ * log₂(pᵢ))\n",
        "Use code with caution\n",
        "where pᵢ is the probability of an element belonging to class i.\n",
        "\n",
        "How Impurity Measures are Used\n",
        "\n",
        "Feature Selection: Decision Trees use impurity measures to select the best feature to split a node. The feature that results in the greatest reduction in impurity is chosen. This reduction in impurity is often referred to as \"information gain.\"\n",
        "Tree Construction: The algorithm recursively splits nodes based on the selected features and their corresponding impurity measures, continuing until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf).\n",
        "Evaluating Splits: Impurity measures help in evaluating the quality of a split by comparing the impurity of the parent node to the impurity of the child nodes.\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "M2Cd0ENd0il1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the mathematical formula for Gini Impurity\n",
        "\n",
        "'''\n",
        "Gini Impurity\n",
        "\n",
        "Gini Impurity is a measure of the probability of misclassifying a randomly chosen element from a set. It's used in Decision Trees to evaluate the quality of a split.\n",
        "\n",
        "Formula\n",
        "\n",
        "The formula for Gini Impurity is:\n",
        "\n",
        "\n",
        "Gini Impurity = 1 - Σ(pᵢ)²\n",
        "Use code with caution\n",
        "where:\n",
        "\n",
        "pᵢ is the probability of an element belonging to class i.\n",
        "The summation (Σ) is taken over all classes in the set.\n",
        "Explanation\n",
        "\n",
        "Probability of Belonging to a Class (pᵢ): For each class i in the set, you calculate the probability of a randomly selected element belonging to that class. This is typically done by dividing the number of elements in class i by the total number of elements in the set.\n",
        "\n",
        "Squaring the Probabilities (pᵢ²): You square the probability of each class.\n",
        "\n",
        "Summing the Squared Probabilities (Σ(pᵢ²)): You sum the squared probabilities of all the classes.\n",
        "\n",
        "Subtracting from 1 (1 - Σ(pᵢ²)): Finally, you subtract the sum of squared probabilities from 1.\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "dvNgKqwI0xrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the mathematical formula for Entropy\n",
        "'''\n",
        "In Decision Trees, Entropy is a measure of the uncertainty or randomness in a set. It's based on information theory and is used to evaluate the quality of a split.\n",
        "\n",
        "Formula\n",
        "\n",
        "The formula for Entropy is:\n",
        "\n",
        "\n",
        "Entropy = -Σ(pᵢ * log₂(pᵢ))\n",
        "Use code with caution\n",
        "where:\n",
        "\n",
        "pᵢ is the probability of an element belonging to class i.\n",
        "The summation (Σ) is taken over all classes in the set.\n",
        "log₂ represents the logarithm base 2.\n",
        "Explanation\n",
        "\n",
        "Probability of Belonging to a Class (pᵢ): For each class i in the set, you calculate the probability of a randomly selected element belonging to that class. This is typically done by dividing the number of elements in class i by the total number of elements in the set.\n",
        "\n",
        "Calculating pᵢ * log₂(pᵢ): For each class, you multiply its probability by the logarithm base 2 of its probability.\n",
        "\n",
        "Summing the Results (Σ(pᵢ * log₂(pᵢ))): You sum the results from step 2 for all classes.\n",
        "\n",
        "Negating the Sum (-Σ(pᵢ * log₂(pᵢ))): Finally, you negate the sum to obtain the Entropy.\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "S_2c0c210xoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is Information Gain, and how is it used in Decision Trees\n",
        "\n",
        "'''\n",
        "Information Gain\n",
        "\n",
        "Information Gain is a concept used in Decision Trees to measure the reduction in entropy (or impurity) achieved by splitting a dataset based on a particular feature. In simpler terms, it quantifies how much information we gain about the target variable by knowing the value of a specific feature.\n",
        "\n",
        "How it Works\n",
        "\n",
        "Calculate Entropy of the Parent Node: First, we calculate the entropy of the entire dataset before any split is made. This represents the initial uncertainty or randomness in the target variable.\n",
        "\n",
        "Calculate Entropy of Child Nodes: Next, we consider splitting the dataset based on a specific feature. We calculate the entropy of each child node resulting from the split.\n",
        "\n",
        "Calculate Weighted Average Entropy: We calculate a weighted average of the child node entropies, where the weights are determined by the proportion of data points in each child node.\n",
        "\n",
        "Calculate Information Gain: Finally, we subtract the weighted average entropy of the child nodes from the entropy of the parent node. This difference represents the Information Gain achieved by the split\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "qhHT7_NC0xlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#What is the difference between Gini Impurity and Entropy\n",
        "\n",
        "'''\n",
        "Both Gini Impurity and Entropy are metrics used to measure the impurity or uncertainty of a node in a Decision Tree. They help the algorithm determine the best feature to split the data at each node, aiming to create the purest possible child nodes.\n",
        "\n",
        "Here's a breakdown of the key differences between the two:\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Definition: Measures the probability of misclassifying a randomly chosen element from a set.\n",
        "Formula: Gini Impurity = 1 - Σ(pᵢ)² (where pᵢ is the probability of an element belonging to class i)\n",
        "Range: 0 (pure node) to 1 (completely impure node)\n",
        "Computation: Generally faster to calculate due to its linear nature.\n",
        "Focus: Emphasizes on creating splits that maximize the probability of correct classification.\n",
        "Entropy\n",
        "\n",
        "Definition: Measures the uncertainty or randomness in a set, based on information theory.\n",
        "Formula: Entropy = -Σ(pᵢ * log₂(pᵢ)) (where pᵢ is the probability of an element belonging to class i)\n",
        "Range: 0 (pure node) to log₂(k) (completely impure node, where k is the number of classes)\n",
        "Computation: Slightly more computationally intensive due to the logarithmic calculation.\n",
        "Focus: Aims to create splits that minimize the uncertainty or randomness in the data.\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "BORoJIzK1b1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the mathematical explanation behind Decision Trees\n",
        "\n",
        "'''\n",
        "Recursive Partitioning: The dataset is recursively divided into subsets based on the values of features, aiming to create increasingly homogeneous subsets with respect to the target variable.\n",
        "\n",
        "Impurity Measures: Metrics like Gini Impurity or Entropy are used to quantify the impurity or uncertainty within a node. The goal is to find splits that minimize impurity and create purer child nodes.\n",
        "\n",
        "Information Gain: The difference in impurity between a parent node and its child nodes after a split is called Information Gain. It helps in selecting the best feature for splitting a node, prioritizing features that maximize Information Gain.\n",
        "\n",
        "Mathematical Formulation\n",
        "\n",
        "Let's consider a dataset D with n instances and m features. The target variable can have k distinct classes.\n",
        "\n",
        "Entropy of a Node: The entropy of a node S is calculated as:\n",
        "\n",
        "Entropy(S) = -Σ(pᵢ * log₂(pᵢ))\n",
        "\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "95mKTTkT1byM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is Pre-Pruning in Decision Trees\n",
        "\n",
        "'''\n",
        "Pre-pruning, also known as early stopping, is a technique used in Decision Trees to prevent overfitting by stopping the tree growth before it becomes too complex. It involves setting constraints or limits during the tree construction process to avoid creating branches that might capture noise or irrelevant patterns in the training data.\n",
        "\n",
        "How it Works\n",
        "\n",
        "Pre-pruning is achieved by using various stopping criteria during tree construction.\n",
        "\n",
        "Maximum Depth: Limiting the maximum depth of the tree, preventing it from growing beyond a certain level.\n",
        "\n",
        "Minimum Samples Split: Specifying the minimum number of samples required to split an internal node. If a node has fewer samples than the threshold, it won't be split further.\n",
        "\n",
        "Minimum Samples Leaf: Setting the minimum number of samples required to be in a leaf node. If a split results in a child node with fewer samples than the threshold, the split is not performed.\n",
        "\n",
        "Maximum Leaf Nodes: Limiting the total number of leaf nodes in the tree.\n",
        "\n",
        "Maximum Features: Restricting the number of features considered for each split.\n",
        "\n",
        "Benefits of Pre-Pruning\n",
        "\n",
        "Reduces Overfitting: By stopping tree growth early, pre-pruning prevents the tree from becoming overly complex and capturing noise in the training data.\n",
        "Improves Generalization: A simpler tree obtained through pre-pruning is more likely to generalize well to unseen data.\n",
        "Reduces Computational Cost: Pre-pruning can reduce the computational cost of building and using the tree, as it limits the number of nodes and branches.\n",
        "Considerations\n",
        "\n",
        "Choosing appropriate pre-pruning parameters requires careful consideration and experimentation.\n",
        "Setting overly restrictive constraints can lead to underfitting, where the tree is too simple to capture the underlying patterns in the data.\n",
        "It's often recommended to use cross-validation to evaluate different pre-pruning settings and find the optimal balance between complexity and generalization.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "HTZcUzKV1bv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is Post-Pruning in Decision Trees\n",
        "\n",
        "'''\n",
        "Post-pruning, also known as backward pruning, is a technique used in Decision Trees to reduce overfitting by removing or collapsing branches after the tree has been fully grown. It involves evaluating the performance of the tree on a validation set and selectively removing branches that do not contribute significantly to the overall accuracy or generalization ability.\n",
        "\n",
        "How it Works\n",
        "\n",
        "Grow the Full Tree: The Decision Tree is first allowed to grow to its full depth without any restrictions.\n",
        "\n",
        "Evaluate on Validation Set: The performance of the tree is assessed on a separate validation set, which is a portion of the training data not used for building the initial tree.\n",
        "\n",
        "Prune Branches: Based on the validation set performance, branches that do not improve or even decrease the accuracy are identified and removed. This can involve collapsing internal nodes into leaf nodes or removing entire subtrees.\n",
        "\n",
        "Stopping Criterion: The pruning process continues until a stopping criterion is met, such as achieving a desired level of accuracy on the validation set or reaching a minimum tree size.\n",
        "\n",
        "Types of Post-Pruning\n",
        "\n",
        "Cost Complexity Pruning: This method assigns a cost to each node based on its complexity and error rate. Branches with high costs and little contribution to accuracy are pruned.\n",
        "\n",
        "Reduced Error Pruning: This approach iteratively removes branches that lead to the most errors on the validation set.\n",
        "\n",
        "Minimum Error Pruning: This technique aims to find the subtree with the lowest error rate on the validation set\n",
        "'''"
      ],
      "metadata": {
        "id": "d-pva_h31bth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the difference between Pre-Pruning and Post-Pruning\n",
        "\n",
        "'''\n",
        "Both pre-pruning and post-pruning are techniques used to address overfitting in Decision Trees, but they differ in their approach and timing:\n",
        "\n",
        "Pre-Pruning\n",
        "\n",
        "Timing: Applied during the tree construction process.\n",
        "Approach: Stops tree growth early by imposing constraints on tree size or structure.\n",
        "Mechanism: Uses stopping criteria like maximum depth, minimum samples split, minimum samples leaf, maximum leaf nodes, or maximum features.\n",
        "Advantages:\n",
        "Reduces overfitting by preventing the tree from becoming too complex.\n",
        "Improves generalization performance on unseen data.\n",
        "Reduces computational cost by limiting tree size.\n",
        "Disadvantages:\n",
        "May lead to underfitting if constraints are too restrictive.\n",
        "Requires careful selection of pre-pruning parameters.\n",
        "Post-Pruning\n",
        "\n",
        "Timing: Applied after the tree has been fully grown.\n",
        "Approach: Removes or collapses branches that do not contribute significantly to accuracy.\n",
        "Mechanism: Evaluates tree performance on a validation set and prunes branches based on accuracy or complexity measures.\n",
        "Advantages:\n",
        "Allows the tree to initially grow to its full potential, potentially capturing more complex relationships.\n",
        "Often leads to more accurate models compared to pre-pruning.\n",
        "More flexible than pre-pruning as it adapts to the data.\n",
        "Disadvantages:\n",
        "Can be computationally more expensive than pre-pruning.\n",
        "Requires a separate validation set for pruning decisions\n",
        "'''\n"
      ],
      "metadata": {
        "id": "2uroLRQo1bqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is a Decision Tree Regressor\n",
        "\n",
        "'''\n",
        "A Decision Tree Regressor is a type of Decision Tree algorithm used for solving regression problems, where the target variable is continuous rather than categorical. It works by recursively partitioning the data into subsets based on features, aiming to create regions where the target variable values are as similar as possible.\n",
        "\n",
        "How it Works\n",
        "\n",
        "Tree Construction: Similar to classification trees, the regressor starts with the root node containing the entire dataset. It then selects the best feature and split point that minimizes the variance within the resulting child nodes. This process is repeated recursively until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf).\n",
        "\n",
        "Prediction: To make a prediction for a new data point, the regressor traverses the tree based on the feature values of the data point until it reaches a leaf node. The predicted value is then the average (or median) of the target variable values in that leaf node.\n",
        "\n",
        "Key Differences from Classification Trees\n",
        "\n",
        "Target Variable: Regressors predict continuous values, while classification trees predict categorical values.\n",
        "Splitting Criterion: Regressors typically use variance reduction (e.g., mean squared error) as the splitting criterion, while classification trees use impurity measures (e.g., Gini impurity, entropy).\n",
        "Prediction: Regressors predict the average (or median) value in a leaf node, while classification trees predict the majority class.\n",
        "Advantages of Decision Tree Regressors\n",
        "\n",
        "Easy to Understand and Interpret: The tree structure provides a clear representation of the decision-making process.\n",
        "Handles Non-linear Relationships: Can capture complex non-linear relationships between features and the target variable.\n",
        "No Feature Scaling Required: Unlike some other regression algorithms, decision tree regressors do not require feature scaling.\n",
        "Handles Outliers: Relatively robust to outliers in the data\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "P0Xm8FTD1boa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dTj6GYS51bmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the advantages and disadvantages of Decision Trees\n",
        "\n",
        "'''\n",
        "Easy to Understand and Interpret: Decision Trees are easy to visualize and understand, making them transparent and interpretable. The tree structure clearly shows the decision-making process, allowing users to trace the path from input features to predictions.\n",
        "\n",
        "Handles Non-linear Relationships: Decision Trees can capture complex non-linear relationships between features and the target variable, unlike linear models that assume linearity. This flexibility allows them to model data with intricate patterns.\n",
        "\n",
        "No Feature Scaling Required: Decision Trees are not sensitive to the scale of features, so there's no need for feature scaling or normalization. This simplifies the data preparation process and makes them robust to variations in feature ranges.\n",
        "\n",
        "Handles Both Categorical and Numerical Data: Decision Trees can handle both categorical and numerical features, making them versatile for various datasets. They can seamlessly incorporate different data types without requiring extensive preprocessing.\n",
        "\n",
        "Handles Outliers: Decision Trees are relatively robust to outliers in the data. Outliers typically have less impact on the tree structure compared to other algorithms, as they are often isolated in separate branches.\n",
        "\n",
        "Feature Importance: Decision Trees can provide insights into the importance of different features in making predictions. By analyzing the tree structure and the features used for splitting, users can gain valuable information about the data.\n",
        "\n",
        "Fast Prediction: Once the tree is built, making predictions is very fast. It involves traversing the tree based on the input features, which is computationally efficient.\n",
        "\n",
        "Disadvantages of Decision Trees\n",
        "\n",
        "Prone to Overfitting: Decision Trees can easily overfit the training data, especially if they are allowed to grow too deep and complex. Overfitting leads to poor generalization performance on unseen data.\n",
        "\n",
        "Instability: Small changes in the data can lead to significant changes in the tree structure, making them unstable. This instability can affect the model's consistency and reliability.\n",
        "\n",
        "Bias towards Features with More Levels: Decision Trees can be biased towards features with more distinct values. Features with many levels tend to have higher information gain, leading to their preferential selection for splits.\n",
        "\n",
        "Difficulty in Capturing Complex Interactions: While Decision Trees can handle non-linear relationships, they might struggle to capture complex interactions between multiple features. They typically consider features individually rather than in combination.\n",
        "\n",
        "Greedy Approach: Decision Trees use a greedy approach for splitting nodes, selecting the best feature at each step without considering future splits. This approach might not always lead to the globally optimal tree structure.\n",
        "\n",
        "Not Ideal for Continuous Targets: While Decision Tree Regressors can handle continuous target variables, they might not be the best choice for highly complex regression problems. They tend to create piecewise constant predictions, which might not accurately capture smooth variations in the target.\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "7LUe9VqR1bjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How does a Decision Tree handle missing values\n",
        "\n",
        "'''\n",
        "Decision Trees have inherent mechanisms for handling missing values during both training and prediction. Here are the common approaches:\n",
        "\n",
        "During Training\n",
        "\n",
        "Ignoring Missing Values: The simplest approach is to ignore instances with missing values for a particular feature when calculating impurity measures or information gain. This can lead to loss of information but is computationally efficient.\n",
        "\n",
        "Imputation with Most Frequent Value: For categorical features, missing values can be replaced with the most frequent value (mode) of that feature in the training data.\n",
        "\n",
        "Imputation with Mean/Median: For numerical features, missing values can be replaced with the mean or median value of that feature in the training data.\n",
        "\n",
        "Creating a Separate Category: For categorical features, a new category can be created to represent missing values. This allows the tree to learn patterns specific to missing data.\n",
        "\n",
        "Surrogate Splits: This approach involves finding a surrogate feature that is highly correlated with the feature containing missing values. When a missing value is encountered, the surrogate feature is used to make the split instead.\n",
        "\n",
        "During Prediction\n",
        "\n",
        "Using Surrogate Splits: If surrogate splits were used during training, the same approach is applied during prediction.\n",
        "\n",
        "Propagating Probabilities: If missing values are encountered during prediction, the tree can propagate the probabilities of different outcomes down multiple branches based on the available features. The final prediction is then a weighted average of the predictions from these branches.\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "gbAPqcLX1bhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIXtoPO11bes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How does a Decision Tree handle categorical features\n",
        "\n",
        "'''\n",
        "Handling Categorical Features in Decision Trees\n",
        "\n",
        "Decision Trees can handle categorical features without requiring one-hot encoding or other preprocessing steps. They achieve this through the following mechanisms:\n",
        "\n",
        "Binary Splits for Categorical Features: For a categorical feature with n distinct values, the Decision Tree algorithm considers all possible binary splits. It evaluates each split by calculating the impurity or information gain resulting from dividing the data into two subsets based on the presence or absence of a particular category.\n",
        "\n",
        "Multi-way Splits for Categorical Features: In some cases, the Decision Tree algorithm might create a multi-way split for a categorical feature, where each distinct value of the feature leads to a separate branch. This is more common when the categorical feature has a small number of distinct values.\n",
        "\n",
        "Impurity Measures for Categorical Features: Impurity measures like Gini impurity or entropy are used to evaluate the quality of splits for categorical features. These measures consider the distribution of different categories within each subset created by a split.\n",
        "\n",
        "Information Gain for Categorical Features: Information gain is used to select the best split for categorical features. The feature and split point that maximize the reduction in impurity (or maximize information gain) are chosen.\n",
        "'''\n"
      ],
      "metadata": {
        "id": "AYuK5tv51bcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What are some real-world applications of Decision Trees\n",
        "\n",
        "'''\n",
        "Customer Relationship Management (CRM):\n",
        "\n",
        "Customer Churn Prediction: Identifying customers who are likely to churn (cancel their subscription or stop using a service) based on their past behavior and demographics.\n",
        "Customer Segmentation: Grouping customers into segments based on their characteristics and preferences to personalize marketing campaigns and product recommendations.\n",
        "Targeted Advertising: Optimizing advertising campaigns by targeting specific customer segments with relevant ads based on their interests and demographics.\n",
        "Healthcare:\n",
        "\n",
        "Disease Diagnosis: Assisting in diagnosing diseases based on patient symptoms, medical history, and test results.\n",
        "Treatment Recommendation: Recommending personalized treatment plans for patients based on their individual characteristics and disease severity.\n",
        "Risk Assessment: Identifying patients at high risk for certain diseases or complications to prioritize preventive care and early interventions.\n",
        "Finance:\n",
        "\n",
        "Credit Scoring: Evaluating the creditworthiness of loan applicants based on their financial history and demographics.\n",
        "Fraud Detection: Identifying fraudulent transactions by analyzing patterns and anomalies in financial data.\n",
        "Investment Strategies: Developing investment strategies by identifying promising stocks or bonds based on market trends and company performance.\n",
        "Marketing and Sales:\n",
        "\n",
        "Product Recommendation: Recommending products to customers based on their past purchases, browsing history, and preferences.\n",
        "Market Segmentation: Identifying distinct customer segments with similar needs and preferences to tailor marketing efforts.\n",
        "Pricing Optimization: Determining optimal pricing strategies for products or services by analyzing market demand and competitor pricing.\n",
        "Operations and Manufacturing:\n",
        "\n",
        "Quality Control: Identifying defects in manufactured products by analyzing sensor data and production parameters.\n",
        "Supply Chain Optimization: Optimizing inventory management and logistics by forecasting demand and predicting potential disruptions.\n",
        "Predictive Maintenance: Predicting equipment failures to schedule maintenance proactively and minimize downtime.\n",
        "Natural Language Processing (NLP):\n",
        "\n",
        "Sentiment Analysis: Classifying text into positive, negative, or neutral sentiments to understand customer feedback or public opinion.\n",
        "Text Classification: Categorizing text documents into predefined categories for spam filtering, topic identification, or document organization.\n",
        "Language Translation: Assisting in language translation by identifying grammatical structures and semantic relationships between words.\n",
        "Image Recognition:\n",
        "\n",
        "Object Detection: Identifying and classifying objects within images for applications like self-driving cars or medical imaging.\n",
        "Image Segmentation: Dividing an image into distinct regions based on object boundaries or pixel characteristics.\n",
        "Facial Recognition: Identifying individuals based on facial features for security or authentication purposes\n",
        "'''"
      ],
      "metadata": {
        "id": "vL3cut7u1bZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Practial**"
      ],
      "metadata": {
        "id": "7a_4Euz6Bicu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_dYd4Sp1bXG",
        "outputId": "fd65edb7-a95c-41b1-be95-ec6ace3cb0d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "patVFxGT1bUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with Gini Impurity\n",
        "tree = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = tree.feature_importances_\n",
        "\n",
        "# Print feature importances\n",
        "for i, importance in enumerate(importances):\n",
        "    print(f\"Feature {i}: {iris.feature_names[i]}, Importance: {importance}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yRmSr821bR7",
        "outputId": "fc255ea4-da40-4826-e2d1-e189872080e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 0: sepal length (cm), Importance: 0.0\n",
            "Feature 1: sepal width (cm), Importance: 0.01911001911001911\n",
            "Feature 2: petal length (cm), Importance: 0.8932635518001373\n",
            "Feature 3: petal width (cm), Importance: 0.08762642908984374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with Entropy\n",
        "tree = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekPeQjo_1bPV",
        "outputId": "909bdf89-b93f-4c17-ef04-508ca4bc9d06"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7B2hWmmrCAJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "# Instead of load_boston, use fetch_california_housing\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the MSE\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aaTBx13CNj0",
        "outputId": "ccd65af5-284c-4ab5-942c-ae90a9cac5a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280096503174904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "\n",
        "\n",
        "!pip install graphviz\n",
        "#Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Export the tree to a DOT file\n",
        "dot_data = export_graphviz(tree, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Visualize the tree using graphviz\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_decision_tree\") # Saves the tree as a PDF file\n",
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XRGN-VP4CVEA",
        "outputId": "d530b33b-ea6b-4b17-bfdf-e98b27fe88af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"749pt\" height=\"790pt\"\n viewBox=\"0.00 0.00 749.00 790.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 786)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-786 745,-786 745,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M265,-782C265,-782 130,-782 130,-782 124,-782 118,-776 118,-770 118,-770 118,-711 118,-711 118,-705 124,-699 130,-699 130,-699 265,-699 265,-699 271,-699 277,-705 277,-711 277,-711 277,-770 277,-770 277,-776 271,-782 265,-782\"/>\n<text text-anchor=\"start\" x=\"126\" y=\"-766.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n<text text-anchor=\"start\" x=\"162\" y=\"-751.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.664</text>\n<text text-anchor=\"start\" x=\"152.5\" y=\"-736.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 105</text>\n<text text-anchor=\"start\" x=\"139.5\" y=\"-721.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 37, 37]</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-706.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M166,-655.5C166,-655.5 73,-655.5 73,-655.5 67,-655.5 61,-649.5 61,-643.5 61,-643.5 61,-599.5 61,-599.5 61,-593.5 67,-587.5 73,-587.5 73,-587.5 166,-587.5 166,-587.5 172,-587.5 178,-593.5 178,-599.5 178,-599.5 178,-643.5 178,-643.5 178,-649.5 172,-655.5 166,-655.5\"/>\n<text text-anchor=\"start\" x=\"91.5\" y=\"-640.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"78.5\" y=\"-625.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 31</text>\n<text text-anchor=\"start\" x=\"69\" y=\"-610.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 0, 0]</text>\n<text text-anchor=\"start\" x=\"76\" y=\"-595.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M170.44,-698.91C162.93,-687.65 154.78,-675.42 147.24,-664.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"150.07,-662.05 141.61,-655.67 144.25,-665.93 150.07,-662.05\"/>\n<text text-anchor=\"middle\" x=\"136.71\" y=\"-676.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M343,-663C343,-663 208,-663 208,-663 202,-663 196,-657 196,-651 196,-651 196,-592 196,-592 196,-586 202,-580 208,-580 208,-580 343,-580 343,-580 349,-580 355,-586 355,-592 355,-592 355,-651 355,-651 355,-657 349,-663 343,-663\"/>\n<text text-anchor=\"start\" x=\"204\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.75</text>\n<text text-anchor=\"start\" x=\"247.5\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"234.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 74</text>\n<text text-anchor=\"start\" x=\"221\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 37, 37]</text>\n<text text-anchor=\"start\" x=\"223\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M224.56,-698.91C230.43,-690.1 236.7,-680.7 242.76,-671.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"245.85,-673.28 248.49,-663.02 240.03,-669.4 245.85,-673.28\"/>\n<text text-anchor=\"middle\" x=\"253.39\" y=\"-683.84\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#3fe685\" stroke=\"black\" d=\"M252.5,-544C252.5,-544 130.5,-544 130.5,-544 124.5,-544 118.5,-538 118.5,-532 118.5,-532 118.5,-473 118.5,-473 118.5,-467 124.5,-461 130.5,-461 130.5,-461 252.5,-461 252.5,-461 258.5,-461 264.5,-467 264.5,-473 264.5,-473 264.5,-532 264.5,-532 264.5,-538 258.5,-544 252.5,-544\"/>\n<text text-anchor=\"start\" x=\"126.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.6</text>\n<text text-anchor=\"start\" x=\"156\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.059</text>\n<text text-anchor=\"start\" x=\"150.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"141\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 32, 1]</text>\n<text text-anchor=\"start\" x=\"139\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.36,-579.91C239.97,-571.01 233.15,-561.51 226.56,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"229.27,-550.1 220.59,-544.02 223.58,-554.19 229.27,-550.1\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#9254e9\" stroke=\"black\" d=\"M424.5,-544C424.5,-544 294.5,-544 294.5,-544 288.5,-544 282.5,-538 282.5,-532 282.5,-532 282.5,-473 282.5,-473 282.5,-467 288.5,-461 294.5,-461 294.5,-461 424.5,-461 424.5,-461 430.5,-461 436.5,-467 436.5,-473 436.5,-473 436.5,-532 436.5,-532 436.5,-538 430.5,-544 424.5,-544\"/>\n<text text-anchor=\"start\" x=\"290.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"324\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.214</text>\n<text text-anchor=\"start\" x=\"318.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 41</text>\n<text text-anchor=\"start\" x=\"309\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 36]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M304.64,-579.91C311.03,-571.01 317.85,-561.51 324.44,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"327.42,-554.19 330.41,-544.02 321.73,-550.1 327.42,-554.19\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-417.5C109,-417.5 12,-417.5 12,-417.5 6,-417.5 0,-411.5 0,-405.5 0,-405.5 0,-361.5 0,-361.5 0,-355.5 6,-349.5 12,-349.5 12,-349.5 109,-349.5 109,-349.5 115,-349.5 121,-355.5 121,-361.5 121,-361.5 121,-405.5 121,-405.5 121,-411.5 115,-417.5 109,-417.5\"/>\n<text text-anchor=\"start\" x=\"32.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 32</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 32, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M146.05,-460.91C132.83,-449.1 118.4,-436.22 105.23,-424.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107.43,-421.72 97.64,-417.67 102.76,-426.94 107.43,-421.72\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-417.5C240,-417.5 151,-417.5 151,-417.5 145,-417.5 139,-411.5 139,-405.5 139,-405.5 139,-361.5 139,-361.5 139,-355.5 145,-349.5 151,-349.5 151,-349.5 240,-349.5 240,-349.5 246,-349.5 252,-355.5 252,-361.5 252,-361.5 252,-405.5 252,-405.5 252,-411.5 246,-417.5 240,-417.5\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.89,-460.91C193.25,-450.2 193.65,-438.62 194.02,-427.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197.52,-427.78 194.37,-417.67 190.53,-427.54 197.52,-427.78\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M424,-425C424,-425 289,-425 289,-425 283,-425 277,-419 277,-413 277,-413 277,-354 277,-354 277,-348 283,-342 289,-342 289,-342 424,-342 424,-342 430,-342 436,-348 436,-354 436,-354 436,-413 436,-413 436,-419 430,-425 424,-425\"/>\n<text text-anchor=\"start\" x=\"285\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n<text text-anchor=\"start\" x=\"328.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"319\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\n<text text-anchor=\"start\" x=\"309.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4, 4]</text>\n<text text-anchor=\"start\" x=\"304\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M358.46,-460.91C358.25,-452.56 358.02,-443.67 357.8,-435.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"361.29,-434.93 357.54,-425.02 354.3,-435.11 361.29,-434.93\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#853fe6\" stroke=\"black\" d=\"M601,-425C601,-425 466,-425 466,-425 460,-425 454,-419 454,-413 454,-413 454,-354 454,-354 454,-348 460,-342 466,-342 466,-342 601,-342 601,-342 607,-342 613,-348 613,-354 613,-354 613,-413 613,-413 613,-419 607,-425 601,-425\"/>\n<text text-anchor=\"start\" x=\"462\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n<text text-anchor=\"start\" x=\"498\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.059</text>\n<text text-anchor=\"start\" x=\"492.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"483\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 32]</text>\n<text text-anchor=\"start\" x=\"485\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 6&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>6&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M419.87,-460.91C434.31,-451.2 449.83,-440.76 464.63,-430.81\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"466.89,-433.51 473.24,-425.02 462.99,-427.7 466.89,-433.51\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M252,-298.5C252,-298.5 155,-298.5 155,-298.5 149,-298.5 143,-292.5 143,-286.5 143,-286.5 143,-242.5 143,-242.5 143,-236.5 149,-230.5 155,-230.5 155,-230.5 252,-230.5 252,-230.5 258,-230.5 264,-236.5 264,-242.5 264,-242.5 264,-286.5 264,-286.5 264,-292.5 258,-298.5 252,-298.5\"/>\n<text text-anchor=\"start\" x=\"175.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"166\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"156.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"151\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M303.42,-341.91C287.69,-329.88 270.5,-316.73 254.88,-304.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"256.94,-301.96 246.87,-298.67 252.69,-307.52 256.94,-301.96\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M424.5,-306C424.5,-306 294.5,-306 294.5,-306 288.5,-306 282.5,-300 282.5,-294 282.5,-294 282.5,-235 282.5,-235 282.5,-229 288.5,-223 294.5,-223 294.5,-223 424.5,-223 424.5,-223 430.5,-223 436.5,-229 436.5,-235 436.5,-235 436.5,-294 436.5,-294 436.5,-300 430.5,-306 424.5,-306\"/>\n<text text-anchor=\"start\" x=\"290.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n<text text-anchor=\"start\" x=\"324\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"322\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"312.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M357.54,-341.91C357.75,-333.56 357.98,-324.67 358.2,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"361.7,-316.11 358.46,-306.02 354.71,-315.93 361.7,-316.11\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M254,-179.5C254,-179.5 165,-179.5 165,-179.5 159,-179.5 153,-173.5 153,-167.5 153,-167.5 153,-123.5 153,-123.5 153,-117.5 159,-111.5 165,-111.5 165,-111.5 254,-111.5 254,-111.5 260,-111.5 266,-117.5 266,-123.5 266,-123.5 266,-167.5 266,-167.5 266,-173.5 260,-179.5 254,-179.5\"/>\n<text text-anchor=\"start\" x=\"181.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"172\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"162.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"161\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M307.46,-222.91C292.18,-210.99 275.49,-197.98 260.29,-186.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"262.06,-183.06 252.02,-179.67 257.75,-188.58 262.06,-183.06\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M431,-187C431,-187 296,-187 296,-187 290,-187 284,-181 284,-175 284,-175 284,-116 284,-116 284,-110 290,-104 296,-104 296,-104 431,-104 431,-104 437,-104 443,-110 443,-116 443,-116 443,-175 443,-175 443,-181 437,-187 431,-187\"/>\n<text text-anchor=\"start\" x=\"292\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.45</text>\n<text text-anchor=\"start\" x=\"328\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"326\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"316.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M360.89,-222.91C361.17,-214.56 361.48,-205.67 361.77,-197.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"365.27,-197.13 362.11,-187.02 358.27,-196.9 365.27,-197.13\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M345,-68C345,-68 248,-68 248,-68 242,-68 236,-62 236,-56 236,-56 236,-12 236,-12 236,-6 242,0 248,0 248,0 345,0 345,0 351,0 357,-6 357,-12 357,-12 357,-56 357,-56 357,-62 351,-68 345,-68\"/>\n<text text-anchor=\"start\" x=\"268.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"259\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"249.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"244\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 11&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>11&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M338.55,-103.73C333.19,-94.97 327.52,-85.7 322.14,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"325.08,-75 316.88,-68.3 319.11,-78.66 325.08,-75\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M476,-68C476,-68 387,-68 387,-68 381,-68 375,-62 375,-56 375,-56 375,-12 375,-12 375,-6 381,0 387,0 387,0 476,0 476,0 482,0 488,-6 488,-12 488,-12 488,-56 488,-56 488,-62 482,-68 476,-68\"/>\n<text text-anchor=\"start\" x=\"403.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"394\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"384.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"383\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 11&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>11&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M388.82,-103.73C394.26,-94.97 400.01,-85.7 405.48,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"408.52,-78.64 410.82,-68.3 402.57,-74.95 408.52,-78.64\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M594,-306C594,-306 469,-306 469,-306 463,-306 457,-300 457,-294 457,-294 457,-235 457,-235 457,-229 463,-223 469,-223 469,-223 594,-223 594,-223 600,-223 606,-229 606,-235 606,-235 606,-294 606,-294 606,-300 600,-306 594,-306\"/>\n<text text-anchor=\"start\" x=\"465\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal width (cm) ≤ 3.1</text>\n<text text-anchor=\"start\" x=\"496\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"494\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"484.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"start\" x=\"483\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>14&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M532.81,-341.91C532.66,-333.56 532.51,-324.67 532.36,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"535.86,-315.96 532.19,-306.02 528.86,-316.08 535.86,-315.96\"/>\n</g>\n<!-- 18 -->\n<g id=\"node19\" class=\"node\">\n<title>18</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M729,-298.5C729,-298.5 636,-298.5 636,-298.5 630,-298.5 624,-292.5 624,-286.5 624,-286.5 624,-242.5 624,-242.5 624,-236.5 630,-230.5 636,-230.5 636,-230.5 729,-230.5 729,-230.5 735,-230.5 741,-236.5 741,-242.5 741,-242.5 741,-286.5 741,-286.5 741,-292.5 735,-298.5 729,-298.5\"/>\n<text text-anchor=\"start\" x=\"654.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"641.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 30</text>\n<text text-anchor=\"start\" x=\"632\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 30]</text>\n<text text-anchor=\"start\" x=\"634\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;18 -->\n<g id=\"edge18\" class=\"edge\">\n<title>14&#45;&gt;18</title>\n<path fill=\"none\" stroke=\"black\" d=\"M585.19,-341.91C600.37,-329.99 616.95,-316.98 632.04,-305.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"634.56,-307.6 640.26,-298.67 630.24,-302.09 634.56,-307.6\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M573,-179.5C573,-179.5 484,-179.5 484,-179.5 478,-179.5 472,-173.5 472,-167.5 472,-167.5 472,-123.5 472,-123.5 472,-117.5 478,-111.5 484,-111.5 484,-111.5 573,-111.5 573,-111.5 579,-111.5 585,-117.5 585,-123.5 585,-123.5 585,-167.5 585,-167.5 585,-173.5 579,-179.5 573,-179.5\"/>\n<text text-anchor=\"start\" x=\"500.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"491\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"481.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"480\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 15&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>15&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M530.46,-222.91C530.18,-212.2 529.89,-200.62 529.61,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"533.11,-189.57 529.35,-179.67 526.11,-189.75 533.11,-189.57\"/>\n</g>\n<!-- 17 -->\n<g id=\"node18\" class=\"node\">\n<title>17</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M712,-179.5C712,-179.5 615,-179.5 615,-179.5 609,-179.5 603,-173.5 603,-167.5 603,-167.5 603,-123.5 603,-123.5 603,-117.5 609,-111.5 615,-111.5 615,-111.5 712,-111.5 712,-111.5 718,-111.5 724,-117.5 724,-123.5 724,-123.5 724,-167.5 724,-167.5 724,-173.5 718,-179.5 712,-179.5\"/>\n<text text-anchor=\"start\" x=\"635.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"626\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"616.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"611\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 15&#45;&gt;17 -->\n<g id=\"edge17\" class=\"edge\">\n<title>15&#45;&gt;17</title>\n<path fill=\"none\" stroke=\"black\" d=\"M577.3,-222.91C590.62,-211.1 605.15,-198.22 618.43,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"620.92,-188.92 626.08,-179.67 616.28,-183.68 620.92,-188.92\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x78d60d8ca0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with maximum depth of 3\n",
        "tree_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model with depth 3\n",
        "tree_depth3.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set (depth 3)\n",
        "y_pred_depth3 = tree_depth3.predict(X_test)\n",
        "\n",
        "# Calculate accuracy (depth 3)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "# Create a fully grown Decision Tree Classifier\n",
        "tree_full = DecisionTreeClassifier(random_state=42)  # No max_depth specified\n",
        "\n",
        "# Train the fully grown model\n",
        "tree_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set (fully grown)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "\n",
        "# Calculate accuracy (fully grown)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy (Depth 3): {accuracy_depth3}\")\n",
        "print(f\"Accuracy (Fully Grown): {accuracy_full}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BrQMCRpCwRi",
        "outputId": "507dbec2-cb2a-4ee0-b89c-af12861803d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Depth 3): 1.0\n",
            "Accuracy (Fully Grown): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FI2GM1pRC9lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "tQIoWn6IC9hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with min_samples_split=5\n",
        "tree_min_split = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "\n",
        "# Train the model with min_samples_split=5\n",
        "tree_min_split.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set (min_samples_split=5)\n",
        "y_pred_min_split = tree_min_split.predict(X_test)\n",
        "\n",
        "# Calculate accuracy (min_samples_split=5)\n",
        "accuracy_min_split = accuracy_score(y_test, y_pred_min_split)\n",
        "\n",
        "# Create a default Decision Tree Classifier\n",
        "tree_default = DecisionTreeClassifier(random_state=42)  # Default min_samples_split=2\n",
        "\n",
        "# Train the default model\n",
        "tree_default.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set (default)\n",
        "y_pred_default = tree_default.predict(X_test)\n",
        "\n",
        "# Calculate accuracy (default)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy (min_samples_split=5): {accuracy_min_split}\")\n",
        "print(f\"Accuracy (Default): {accuracy_default}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABoJWLprC9gB",
        "outputId": "b921e7ed-2706-43b0-f42c-94aa0736b555"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (min_samples_split=5): 1.0\n",
            "Accuracy (Default): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XY6Rvc9dC9eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train a Decision Tree Classifier with unscaled data\n",
        "tree_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "tree_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = tree_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply feature scaling using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Decision Tree Classifier with scaled data\n",
        "tree_scaled = DecisionTreeClassifier(random_state=42)\n",
        "tree_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = tree_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy (Unscaled Data): {accuracy_unscaled}\")\n",
        "print(f\"Accuracy (Scaled Data): {accuracy_scaled}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InbKzwraC9cJ",
        "outputId": "25fc5c8b-39cb-49a8-9850-00bc07946585"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Unscaled Data): 1.0\n",
            "Accuracy (Scaled Data): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJJuIKncC9Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "base_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create an OvR classifier using the Decision Tree as the base estimator\n",
        "ovr_classifier = OneVsRestClassifier(base_tree)\n",
        "\n",
        "# Train the OvR classifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy (One-vs-Rest): {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXqCUEXBC9XP",
        "outputId": "647a1d61-59ce-4089-bafc-9e3f62919eb3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (One-vs-Rest): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T0FcxtKYC9U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "53gG07BkC9Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZpqYToKqC9Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier and display the feature importance scores\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = tree.feature_importances_\n",
        "\n",
        "# Print feature importances along with their names\n",
        "for i, importance in enumerate(importances):\n",
        "    print(f\"Feature {iris.feature_names[i]}: {importance}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCJ2qkRcC9Oi",
        "outputId": "ad1980eb-bc49-4258-d3e1-0b3234a55a5a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature sepal length (cm): 0.0\n",
            "Feature sepal width (cm): 0.01911001911001911\n",
            "Feature petal length (cm): 0.8932635518001373\n",
            "Feature petal width (cm): 0.08762642908984374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qyq805UXC9MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor with max_depth=5\n",
        "regressor_depth5 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "\n",
        "# Train the model with max_depth=5\n",
        "regressor_depth5.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set (max_depth=5)\n",
        "y_pred_depth5 = regressor_depth5.predict(X_test)\n",
        "\n",
        "# Calculate MSE (max_depth=5)\n",
        "mse_depth5 = mean_squared_error(y_test, y_pred_depth5)\n",
        "\n",
        "# Create an unrestricted Decision Tree Regressor\n",
        "regressor_unrestricted = DecisionTreeRegressor(random_state=42)  # No max_depth specified\n",
        "\n",
        "# Train the unrestricted model\n",
        "regressor_unrestricted.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set (unrestricted)\n",
        "y_pred_unrestricted = regressor_unrestricted.predict(X_test)\n",
        "\n",
        "# Calculate MSE (unrestricted)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "# Print the MSE values\n",
        "print(f\"MSE (max_depth=5): {mse_depth5}\")\n",
        "print(f\"MSE (Unrestricted): {mse_unrestricted}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0LBSRubC9J_",
        "outputId": "e8be235a-4afd-4933-e117-96a7f6950061"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE (max_depth=5): 0.5210801561811793\n",
            "MSE (Unrestricted): 0.5280096503174904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rxRHOGDtC9Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy\n",
        "\n",
        "\n",
        "!pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Get the path to the tree\n",
        "path = tree.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Create a list to store the accuracies for different alpha values\n",
        "accuracies = []\n",
        "\n",
        "# Iterate over different alpha values\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    # Create a new tree with the current alpha value\n",
        "    pruned_tree = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "\n",
        "    # Train the pruned tree\n",
        "    pruned_tree.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = pruned_tree.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy and add it to the list\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Plot the accuracy vs alpha\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, accuracies, marker='o', drawstyle=\"steps-post\")\n",
        "plt.xlabel(\"alpha\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "N7TqwrqDC9FX",
        "outputId": "7315dbf8-10d4-4d09-ff3e-f238404d7fbd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'alpha')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAINCAYAAAD2uQoVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAL9lJREFUeJzt3X9413W9+P/HNt0myYaIbBM54K9UQiHhQKtMPc0gjbSrc6JfgruSjh67jrmsJJUVlbM0Dl1eJMUlxw5eJ6iO1enSa1o7h/pqFAZyCjHzBwoWGyC6IcTQ7fX9w4+zxQZ7w37Ac7fbdb0v2WvP1+v9fPt0vr37er1fy8uyLAsAAICE5A/0BAAAAHqb0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUcN9AR6or29Pf785z/H0KFDIy8vb6CnAwAADJAsy2Lnzp1x4oknRn5+9+dtjojQ+fOf/xyjR48e6GkAAACHic2bN8dJJ53U7fePiNAZOnRoRLz2YkpKSgZ4NgAAwEBpaWmJ0aNHdzRCd46I0Hn9crWSkhKhAwAAHPAjLW5GAAAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJOeogZ7AkaStPYvVG3fE1p17YuTQ4phy8vAoyM/r02PufbU9lq16Np7bsTvGDB8Sl1eOjcKj9t+nfTFPAAA4kuQcOr/85S/jtttuizVr1sSWLVviRz/6UVx22WX73WflypVRU1MTjz32WIwePTpuuummuOKKKw5yygOjfv2W+NJPN8SW5j0d2ypKi6N2xriYPr6iT45Zd/+GWPL/bYz27I19vnr/4zHnvJNj7sXj+m2eAABwpMn50rVdu3bFhAkTYtGiRT0av3HjxrjkkkviwgsvjHXr1sWnP/3puPLKK+OBBx7IebIDpX79lrj6nrWd4iEiorF5T1x9z9qoX7+l14855z8eiW//snPkRES0ZxHf/uXGqLt/Q7/MEwAAjkR5WZZlBx7Wzc55eQc8o/P5z38+7rvvvli/fn3Htg9/+MPx0ksvRX19fY+ep6WlJUpLS6O5uTlKSkoOdroHpa09i3d+7X/2iYfX5UVEWUlx/KzmXT2+PKytPYuqBb+IppbWg55Xfl7E2psv6riM7UDHPJh5/rVjji6IvDyXvwEAMLB62gZ9/hmdVatWRVVVVadt06ZNi09/+tPd7tPa2hqtrW/8B3tLS0tfTe+AVm/c0W3kRERkEdHYsifO/uKD/TepeO3MzsT5P+vx+EOd5+Qxx8UPrqoUOwAAHBH6/K5rjY2NUVZW1mlbWVlZtLS0xF/+8pcu96mrq4vS0tKOx+jRo/t6mt3aurP7yBlMfvvci/GXV9oGehoAANAjh+Vd1+bOnRs1NTUdX7e0tAxY7IwcWtyjcXdX/31MOXl4j8au3rgjrvj3Rw5lWhER8fnpZ8Tst4/N6Zi5zDMiYvfetpj8lZ8f7BQBAGBA9HnolJeXR1NTU6dtTU1NUVJSEsccc0yX+xQVFUVRUVFfT61Hppw8PCpKi6OxeU909WGmvIgoLy2O804/oceffTnv9BP2e8yeyM+L+MQ7T+n4jM6Bjnkw8wQAgCNVn1+6VllZGQ0NDZ22/exnP4vKysq+fupeUZCfF7UzXruV89/mwetf184Yl1M8HOiYeRFx0biR+z3GnPNO7vT7dPpingAAcKTKOXRefvnlWLduXaxbty4iXrt99Lp162LTpk0R8dplZ7NmzeoYf9VVV8UzzzwTn/vc5+IPf/hDfOtb34rvf//7cd111/XOK+gH08dXxJ0fPzfKSztfxlZeWhx3fvzcg/r9NAc65pJZfx///K6T42+7JD8v4p/f1fXv0emLeQIAwJEo59tLr1y5Mi688MJ9ts+ePTvuvvvuuOKKK+LZZ5+NlStXdtrnuuuuiw0bNsRJJ50UN998c06/MHQgby/919ras1i9cUds3bknRg4tjiknDz/kMyQHOubeV9tj2apn47kdu2PM8CFxeeXYTmdy+nqeu/e+GuPmvfY7jzbMnxZDCg/Lj3UBADBI9LQNDun36PSXwyV0BiOhAwDA4aSnbdDnn9EBAADob0KH/Wprf+OE3+qNOzp9DQAAhyuhQ7fq12+JqgW/6Pj6in9/JN75tf+J+vVbBnBWAABwYEKHLtWv3xJX37M2mlpaO21vbN4TV9+zVuwAAHBY88ly9tHWnsWXfrqhy188msVrv5fni/+9Id5x2gi/lwcAjmDHHF0QeXney0mT0GEfqzfuiC3Ne7r9fhYRjS174uwvPth/kwIAet3kMcfFD66qFDskyaVr7GPrzu4jBwBIx2+fezH+8krbQE8D+oQzOuxj5NDiHo27u/rvY8rJw/t4NgBAb9u9ty0mf+XnAz0N6FNCh31MOXl4VJQWR2Pzni4/p5MXEeWlxXHe6Sf4jA4AAIcll66xj4L8vKidMS4iXouav/b617UzxokcAAAOW0KHLk0fXxF3fvzcKC/tfBlbeWlx3Pnxc2P6+IoBmhkAAByYS9fo1vTxFXHRuPJYvXFHbN25J0YOLY4pJw93JgcAgMOe0GG/CvLzovLU4wd6GgAAkBOXrgEAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJCcgwqdRYsWxdixY6O4uDimTp0aq1ev7nbsK6+8EvPnz49TTz01iouLY8KECVFfX3/QEwYAADiQnENnxYoVUVNTE7W1tbF27dqYMGFCTJs2LbZu3drl+Jtuuim+/e1vxx133BEbNmyIq666Kj7wgQ/Eo48+esiTBwAA6ErOobNgwYKYM2dOVFdXx7hx42Lx4sUxZMiQWLp0aZfjly1bFl/4whfi4osvjlNOOSWuvvrquPjii+Mb3/jGIU8eAACgKzmFzt69e2PNmjVRVVX1xgHy86OqqipWrVrV5T6tra1RXFzcadsxxxwTDz30ULfP09raGi0tLZ0eAAAAPZVT6Gzfvj3a2tqirKys0/aysrJobGzscp9p06bFggUL4sknn4z29vb42c9+Fvfee29s2bKl2+epq6uL0tLSjsfo0aNzmSYAADDI9fld1775zW/G6aefHmeeeWYUFhbGpz71qaiuro78/O6feu7cudHc3Nzx2Lx5c19PEwAASEhOoTNixIgoKCiIpqamTtubmpqivLy8y31OOOGE+PGPfxy7du2K5557Lv7whz/EscceG6ecckq3z1NUVBQlJSWdHgAAAD2VU+gUFhbGpEmToqGhoWNbe3t7NDQ0RGVl5X73LS4ujlGjRsWrr74a//Vf/xWXXnrpwc0YAADgAI7KdYeampqYPXt2TJ48OaZMmRILFy6MXbt2RXV1dUREzJo1K0aNGhV1dXUREfGb3/wm/vSnP8XEiRPjT3/6U3zxi1+M9vb2+NznPte7rwQAAOD/yTl0Zs6cGdu2bYt58+ZFY2NjTJw4Merr6ztuULBp06ZOn7/Zs2dP3HTTTfHMM8/EscceGxdffHEsW7Yshg0b1msvAgAA4K/lZVmWDfQkDqSlpSVKS0ujubnZ53UAAA7R7r2vxrh5D0RExIb502JIYc7/7xsGTE/boM/vugYAANDfhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJOajQWbRoUYwdOzaKi4tj6tSpsXr16v2OX7hwYZxxxhlxzDHHxOjRo+O6666LPXv2HNSEAQAADiTn0FmxYkXU1NREbW1trF27NiZMmBDTpk2LrVu3djn+P//zP+OGG26I2traePzxx+Ouu+6KFStWxBe+8IVDnjwAAEBXcg6dBQsWxJw5c6K6ujrGjRsXixcvjiFDhsTSpUu7HP+rX/0q3vGOd8RHP/rRGDt2bLznPe+Jj3zkIwc8CwQAAHCwcgqdvXv3xpo1a6KqquqNA+TnR1VVVaxatarLfd7+9rfHmjVrOsLmmWeeifvvvz8uvvjibp+ntbU1WlpaOj0AAAB66qhcBm/fvj3a2tqirKys0/aysrL4wx/+0OU+H/3oR2P79u3xzne+M7Isi1dffTWuuuqq/V66VldXF1/60pdymRoAAECHPr/r2sqVK+OWW26Jb33rW7F27dq4995747777osvf/nL3e4zd+7caG5u7nhs3ry5r6cJAAAkJKczOiNGjIiCgoJoamrqtL2pqSnKy8u73Ofmm2+Oyy+/PK688sqIiDj77LNj165d8clPfjJuvPHGyM/ft7WKioqiqKgol6kBAAB0yOmMTmFhYUyaNCkaGho6trW3t0dDQ0NUVlZ2uc/u3bv3iZmCgoKIiMiyLNf5AgAAHFBOZ3QiImpqamL27NkxefLkmDJlSixcuDB27doV1dXVERExa9asGDVqVNTV1UVExIwZM2LBggXx1re+NaZOnRpPPfVU3HzzzTFjxoyO4AEAAOhNOYfOzJkzY9u2bTFv3rxobGyMiRMnRn19fccNCjZt2tTpDM5NN90UeXl5cdNNN8Wf/vSnOOGEE2LGjBnx1a9+tfdeBQAAwF/Jy46A68daWlqitLQ0mpubo6SkZKCnAwBwRNu999UYN++BiIjYMH9aDCnM+f99w4DpaRv0+V3XAAAA+pvQAQAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEiO0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEiO0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEiO0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUIHAGCQaWvPOv68euOOTl9DKoQOAMAgUr9+S1Qt+EXH11f8+yPxzq/9T9Sv3zKAs4LeJ3QAAAaJ+vVb4up71kZTS2un7Y3Ne+Lqe9aKHZJy1EBPAACAvtfWnsWXfrohurpILYuIvIj44n9viHecNiIK8vP6eXYc7o45uiDy8o6sfy6EDgDAILB6447Y0ryn2+9nEdHYsifO/uKD/TcpjhiTxxwXP7iq8oiKHZeuAQAMAlt3dh85cCC/fe7F+MsrbQM9jZw4owMAMAiMHFrco3F3V/99TDl5eB/PhiPF7r1tMfkrPx/oaRwUoQMAMAhMOXl4VJQWR2Pzni4/p5MXEeWlxXHe6Sf4jA5JOKhL1xYtWhRjx46N4uLimDp1aqxevbrbsRdccEHk5eXt87jkkksOetIAAOSmID8vameMi4jXouavvf517YxxIodk5Bw6K1asiJqamqitrY21a9fGhAkTYtq0abF169Yux997772xZcuWjsf69eujoKAg/umf/umQJw8AQM9NH18Rd3783Cgv7XwZW3lpcdz58XNj+viKAZoZ9L6cL11bsGBBzJkzJ6qrqyMiYvHixXHffffF0qVL44Ybbthn/PDhna/xXL58eQwZMkToAAAMgOnjK+KiceWxeuOO2LpzT4wcWhxTTh7uTA7JySl09u7dG2vWrIm5c+d2bMvPz4+qqqpYtWpVj45x1113xYc//OF405ve1O2Y1tbWaG194xdZtbS05DJNAAD2oyA/LypPPX6gpwF9KqdL17Zv3x5tbW1RVlbWaXtZWVk0NjYecP/Vq1fH+vXr48orr9zvuLq6uigtLe14jB49OpdpAgAAg1y//h6du+66K84+++yYMmXKfsfNnTs3mpubOx6bN2/upxkCAAApyOnStREjRkRBQUE0NTV12t7U1BTl5eX73XfXrl2xfPnymD9//gGfp6ioKIqKinKZGgAAQIeczugUFhbGpEmToqGhoWNbe3t7NDQ0RGVl5X73/cEPfhCtra3x8Y9//OBmCgAA0EM533WtpqYmZs+eHZMnT44pU6bEwoULY9euXR13YZs1a1aMGjUq6urqOu131113xWWXXRbHH++DbwAAQN/KOXRmzpwZ27Zti3nz5kVjY2NMnDgx6uvrO25QsGnTpsjP73yi6IknnoiHHnooHnzwwd6ZNQAAwH7kZVmWDfQkDqSlpSVKS0ujubk5SkpKBno6AAAwKOze+2qMm/dARERsmD8thhTmfJ6k1/W0Dfr1rmsAAAD9QegAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQnIMKnUWLFsXYsWOjuLg4pk6dGqtXr97v+JdeeimuueaaqKioiKKionjzm98c999//0FNGAAA4ECOynWHFStWRE1NTSxevDimTp0aCxcujGnTpsUTTzwRI0eO3Gf83r1746KLLoqRI0fGD3/4wxg1alQ899xzMWzYsN6YPwAAwD5yDp0FCxbEnDlzorq6OiIiFi9eHPfdd18sXbo0brjhhn3GL126NHbs2BG/+tWv4uijj46IiLFjxx7arAEAAPYjp0vX9u7dG2vWrImqqqo3DpCfH1VVVbFq1aou9/nv//7vqKysjGuuuSbKyspi/Pjxccstt0RbW1u3z9Pa2hotLS2dHgAAAD2VU+hs37492traoqysrNP2srKyaGxs7HKfZ555Jn74wx9GW1tb3H///XHzzTfHN77xjfjKV77S7fPU1dVFaWlpx2P06NG5TBMAABjk+vyua+3t7TFy5Mj4zne+E5MmTYqZM2fGjTfeGIsXL+52n7lz50Zzc3PHY/PmzX09TQAAICE5fUZnxIgRUVBQEE1NTZ22NzU1RXl5eZf7VFRUxNFHHx0FBQUd284666xobGyMvXv3RmFh4T77FBUVRVFRUS5TAwAA6JDTGZ3CwsKYNGlSNDQ0dGxrb2+PhoaGqKys7HKfd7zjHfHUU09Fe3t7x7Y//vGPUVFR0WXkAAAAHKqcL12rqamJJUuWxHe/+914/PHH4+qrr45du3Z13IVt1qxZMXfu3I7xV199dezYsSOuvfba+OMf/xj33Xdf3HLLLXHNNdf03qsAAAD4KznfXnrmzJmxbdu2mDdvXjQ2NsbEiROjvr6+4wYFmzZtivz8N/pp9OjR8cADD8R1110X55xzTowaNSquvfba+PznP997rwIAAOCv5GVZlg30JA6kpaUlSktLo7m5OUpKSgZ6OgAAMCjs3vtqjJv3QEREbJg/LYYU5nyepNf1tA36/K5rAAAA/U3oAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkJyDCp1FixbF2LFjo7i4OKZOnRqrV6/uduzdd98deXl5nR7FxcUHPWEAAIADyTl0VqxYETU1NVFbWxtr166NCRMmxLRp02Lr1q3d7lNSUhJbtmzpeDz33HOHNGkAAID9yTl0FixYEHPmzInq6uoYN25cLF68OIYMGRJLly7tdp+8vLwoLy/veJSVlR3SpAEAAPYnp9DZu3dvrFmzJqqqqt44QH5+VFVVxapVq7rd7+WXX44xY8bE6NGj49JLL43HHntsv8/T2toaLS0tnR4AAAA9lVPobN++Pdra2vY5I1NWVhaNjY1d7nPGGWfE0qVL4yc/+Uncc8890d7eHm9/+9vj+eef7/Z56urqorS0tOMxevToXKYJAAAMcn1+17XKysqYNWtWTJw4Mc4///y4995744QTTohvf/vb3e4zd+7caG5u7nhs3ry5r6cJAAAk5KhcBo8YMSIKCgqiqamp0/ampqYoLy/v0TGOPvroeOtb3xpPPfVUt2OKioqiqKgol6kBAAB0yOmMTmFhYUyaNCkaGho6trW3t0dDQ0NUVlb26BhtbW3x+9//PioqKnKbKQAAQA/ldEYnIqKmpiZmz54dkydPjilTpsTChQtj165dUV1dHRERs2bNilGjRkVdXV1ERMyfPz/e9ra3xWmnnRYvvfRS3HbbbfHcc8/FlVde2buvBAAA4P/JOXRmzpwZ27Zti3nz5kVjY2NMnDgx6uvrO25QsGnTpsjPf+NE0Ysvvhhz5syJxsbGOO6442LSpEnxq1/9KsaNG9d7rwIAAOCv5GVZlg30JA6kpaUlSktLo7m5OUpKSgZ6OgAAMCjs3vtqjJv3QEREbJg/LYYU5nyepNf1tA36/K5rAAAA/U3oAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkJyDCp1FixbF2LFjo7i4OKZOnRqrV6/u0X7Lly+PvLy8uOyyyw7maQEAAHok59BZsWJF1NTURG1tbaxduzYmTJgQ06ZNi61bt+53v2effTauv/76OO+88w56sgAAAD2Rc+gsWLAg5syZE9XV1TFu3LhYvHhxDBkyJJYuXdrtPm1tbfGxj30svvSlL8Upp5xySBMGAAA4kJxCZ+/evbFmzZqoqqp64wD5+VFVVRWrVq3qdr/58+fHyJEj4xOf+ESPnqe1tTVaWlo6PQAAAHoqp9DZvn17tLW1RVlZWaftZWVl0djY2OU+Dz30UNx1112xZMmSHj9PXV1dlJaWdjxGjx6dyzQBAIBBrk/vurZz5864/PLLY8mSJTFixIge7zd37txobm7ueGzevLkPZwkAAKTmqFwGjxgxIgoKCqKpqanT9qampigvL99n/NNPPx3PPvtszJgxo2Nbe3v7a0981FHxxBNPxKmnnrrPfkVFRVFUVJTL1AAAADrkdEansLAwJk2aFA0NDR3b2tvbo6GhISorK/cZf+aZZ8bvf//7WLduXcfj/e9/f1x44YWxbt06l6QBAAB9IqczOhERNTU1MXv27Jg8eXJMmTIlFi5cGLt27Yrq6uqIiJg1a1aMGjUq6urqori4OMaPH99p/2HDhkVE7LMdAACgt+QcOjNnzoxt27bFvHnzorGxMSZOnBj19fUdNyjYtGlT5Of36Ud/AAAA9isvy7JsoCdxIC0tLVFaWhrNzc1RUlIy0NMBAIBBYffeV2PcvAciImLD/GkxpDDn8yS9rqdt4NQLAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcg4qdBYtWhRjx46N4uLimDp1aqxevbrbsffee29Mnjw5hg0bFm9605ti4sSJsWzZsoOeMAAAwIHkHDorVqyImpqaqK2tjbVr18aECRNi2rRpsXXr1i7HDx8+PG688cZYtWpV/O53v4vq6uqorq6OBx544JAnDwAA0JWcQ2fBggUxZ86cqK6ujnHjxsXixYtjyJAhsXTp0i7HX3DBBfGBD3wgzjrrrDj11FPj2muvjXPOOSceeuihQ548AABAV3IKnb1798aaNWuiqqrqjQPk50dVVVWsWrXqgPtnWRYNDQ3xxBNPxLve9a5ux7W2tkZLS0unBwAAQE/lFDrbt2+Ptra2KCsr67S9rKwsGhsbu92vubk5jj322CgsLIxLLrkk7rjjjrjooou6HV9XVxelpaUdj9GjR+cyTQAAYJDrl7uuDR06NNatWxePPPJIfPWrX42amppYuXJlt+Pnzp0bzc3NHY/Nmzf3xzQBAIBEHJXL4BEjRkRBQUE0NTV12t7U1BTl5eXd7pefnx+nnXZaRERMnDgxHn/88airq4sLLrigy/FFRUVRVFSUy9QAAIBe1taedfx59cYdcd7pJ0RBft4AzqjncjqjU1hYGJMmTYqGhoaObe3t7dHQ0BCVlZU9Pk57e3u0trbm8tQAAEA/ql+/JaoW/KLj6yv+/ZF459f+J+rXbxnAWfVcTmd0IiJqampi9uzZMXny5JgyZUosXLgwdu3aFdXV1RERMWvWrBg1alTU1dVFxGuft5k8eXKceuqp0draGvfff38sW7Ys7rzzzt59JQAAQK+oX78lrr5nbWR/s72xeU9cfc/auPPj58b08RUDMreeyjl0Zs6cGdu2bYt58+ZFY2NjTJw4Merr6ztuULBp06bIz3/jRNGuXbviX/7lX+L555+PY445Js4888y45557YubMmb33KgAAgF7R1p7Fl366YZ/IiYjIIiIvIr700w1x0bjyw/oytrwsy7p6DYeVlpaWKC0tjebm5igpKRno6QAAQLJWPf1CfGTJrw847ntz3haVpx7fDzPqrKdt0C93XQMAAI4MW3fu6dVxA0XoAAAAHUYOLe7VcQNF6AAAAB2mnDw8KkqLo7tP3+RFREVpcUw5eXh/TitnQgcAAOhQkJ8XtTPGRUTsEzuvf107Y9xhfSOCCKEDAAD8jenjK+LOj58b5aWdL08rLy0+Im4tHXEQt5cGAADSN318RVw0rjxWb9wRW3fuiZFDX7tc7XA/k/M6oQMAAHSpID9vQG4h3RtcugYAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACTnqIGeQE9kWRYRES0tLQM8EwAAYCC93gSvN0J3jojQ2blzZ0REjB49eoBnAgAAHA527twZpaWl3X4/LztQCh0G2tvb489//nMMHTo08vLyBnQuLS0tMXr06Ni8eXOUlJQM6Fw4ONbwyGcNj3zW8Mhm/Y581vDIN5jXMMuy2LlzZ5x44omRn9/9J3GOiDM6+fn5cdJJJw30NDopKSkZdP9QpcYaHvms4ZHPGh7ZrN+Rzxoe+QbrGu7vTM7r3IwAAABIjtABAACSI3RyVFRUFLW1tVFUVDTQU+EgWcMjnzU88lnDI5v1O/JZwyOfNTywI+JmBAAAALlwRgcAAEiO0AEAAJIjdAAAgOQIHQAAIDlCJyIWLVoUY8eOjeLi4pg6dWqsXr16v+N/8IMfxJlnnhnFxcVx9tlnx/3339/p+1mWxbx586KioiKOOeaYqKqqiieffLIvX8Kg1tvrd8UVV0ReXl6nx/Tp0/vyJQx6uazhY489Fh/84Adj7NixkZeXFwsXLjzkY3LoensNv/jFL+7zc3jmmWf24SsglzVcsmRJnHfeeXHcccfFcccdF1VVVfuM917Y/3p7Db0f9r9c1vDee++NyZMnx7Bhw+JNb3pTTJw4MZYtW9ZpzKD/OcwGueXLl2eFhYXZ0qVLs8ceeyybM2dONmzYsKypqanL8Q8//HBWUFCQff3rX882bNiQ3XTTTdnRRx+d/f73v+8Yc+utt2alpaXZj3/84+z//u//sve///3ZySefnP3lL3/pr5c1aPTF+s2ePTubPn16tmXLlo7Hjh07+uslDTq5ruHq1auz66+/Pvve976XlZeXZ//2b/92yMfk0PTFGtbW1mZvectbOv0cbtu2rY9fyeCV6xp+9KMfzRYtWpQ9+uij2eOPP55dccUVWWlpafb88893jPFe2L/6Yg29H/avXNfwf//3f7N7770327BhQ/bUU09lCxcuzAoKCrL6+vqOMYP953DQh86UKVOya665puPrtra27MQTT8zq6uq6HP+hD30ou+SSSzptmzp1avbP//zPWZZlWXt7e1ZeXp7ddtttHd9/6aWXsqKioux73/teH7yCwa231y/LXvsX+6WXXton82Vfua7hXxszZkyX/5F8KMckd32xhrW1tdmECRN6cZbsz6H+zLz66qvZ0KFDs+9+97tZlnkvHAi9vYZZ5v2wv/XGe9db3/rW7KabbsqyzM9hlmXZoL50be/evbFmzZqoqqrq2Jafnx9VVVWxatWqLvdZtWpVp/EREdOmTesYv3HjxmhsbOw0prS0NKZOndrtMTk4fbF+r1u5cmWMHDkyzjjjjLj66qvjhRde6P0XwEGt4UAck+715d/vJ598Mk488cQ45ZRT4mMf+1hs2rTpUKdLF3pjDXfv3h2vvPJKDB8+PCK8F/a3vljD13k/7B+HuoZZlkVDQ0M88cQT8a53vSsi/BxGDPLP6Gzfvj3a2tqirKys0/aysrJobGzscp/Gxsb9jn/9r7kck4PTF+sXETF9+vT4j//4j2hoaIivfe1r8Ytf/CLe+973RltbW++/iEHuYNZwII5J9/rq7/fUqVPj7rvvjvr6+rjzzjtj48aNcd5558XOnTsPdcr8jd5Yw89//vNx4okndvwHlffC/tUXaxjh/bA/HewaNjc3x7HHHhuFhYVxySWXxB133BEXXXRRRPg5jIg4aqAnAIebD3/4wx1/Pvvss+Occ86JU089NVauXBnvfve7B3BmMHi8973v7fjzOeecE1OnTo0xY8bE97///fjEJz4xgDPjb916662xfPnyWLlyZRQXFw/0dDgI3a2h98PD39ChQ2PdunXx8ssvR0NDQ9TU1MQpp5wSF1xwwUBP7bAwqM/ojBgxIgoKCqKpqanT9qampigvL+9yn/Ly8v2Of/2vuRyTg9MX69eVU045JUaMGBFPPfXUoU+aTg5mDQfimHSvv/5+Dxs2LN785jf7OewDh7KGt99+e9x6663x4IMPxjnnnNOx3Xth/+qLNeyK98O+c7BrmJ+fH6eddlpMnDgxPvOZz8Q//uM/Rl1dXUT4OYwY5KFTWFgYkyZNioaGho5t7e3t0dDQEJWVlV3uU1lZ2Wl8RMTPfvazjvEnn3xylJeXdxrT0tISv/nNb7o9JgenL9avK88//3y88MILUVFR0TsTp8PBrOFAHJPu9dff75dffjmefvppP4d94GDX8Otf/3p8+ctfjvr6+pg8eXKn73kv7F99sYZd8X7Yd3rr36Xt7e3R2toaEX4OI8LtpZcvX54VFRVld999d7Zhw4bsk5/8ZDZs2LCssbExy7Isu/zyy7MbbrihY/zDDz+cHXXUUdntt9+ePf7441ltbW2Xt5ceNmxY9pOf/CT73e9+l1166aWD6lZ+/am312/nzp3Z9ddfn61atSrbuHFj9vOf/zw799xzs9NPPz3bs2fPgLzG1OW6hq2trdmjjz6aPfroo1lFRUV2/fXXZ48++mj25JNP9viY9K6+WMPPfOYz2cqVK7ONGzdmDz/8cFZVVZWNGDEi27p1a7+/vsEg1zW89dZbs8LCwuyHP/xhp1sP79y5s9MY74X9p7fX0Pth/8t1DW+55ZbswQcfzJ5++ulsw4YN2e23354dddRR2ZIlSzrGDPafw0EfOlmWZXfccUf2d3/3d1lhYWE2ZcqU7Ne//nXH984///xs9uzZncZ///vfz9785jdnhYWF2Vve8pbsvvvu6/T99vb27Oabb87KysqyoqKi7N3vfnf2xBNP9MdLGZR6c/12796dvec978lOOOGE7Oijj87GjBmTzZkzx38g97Fc1nDjxo1ZROzzOP/883t8THpfb6/hzJkzs4qKiqywsDAbNWpUNnPmzOypp57qx1c0+OSyhmPGjOlyDWtrazvGeC/sf725ht4PB0Yua3jjjTdmp512WlZcXJwdd9xxWWVlZbZ8+fJOxxvsP4d5WZZl/XsOCQAAoG8N6s/oAAAAaRI6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQPAgHv22WcjLy8v1q1b1+N97r777hg2bFifzQmAI5vQAQAAkiN0AACA5AgdAPpFfX19vPOd74xhw4bF8ccfH+973/vi6aef7nLsypUrIy8vL+67774455xzori4ON72trfF+vXr9xn7wAMPxFlnnRXHHntsTJ8+PbZs2dLxvUceeSQuuuiiGDFiRJSWlsb5558fa9eu7bPXCMDhQ+gA0C927doVNTU18dvf/jYaGhoiPz8/PvCBD0R7e3u3+3z2s5+Nb3zjG/HII4/ECSecEDNmzIhXXnml4/u7d++O22+/PZYtWxa//OUvY9OmTXH99dd3fH/nzp0xe/bseOihh+LXv/51nH766XHxxRfHzp07+/S1AjDwjhroCQAwOHzwgx/s9PXSpUvjhBNOiA0bNsSxxx7b5T61tbVx0UUXRUTEd7/73TjppJPiRz/6UXzoQx+KiIhXXnklFi9eHKeeempERHzqU5+K+fPnd+z/D//wD52O953vfCeGDRsWv/jFL+J973tfr702AA4/zugA0C+efPLJ+MhHPhKnnHJKlJSUxNixYyMiYtOmTd3uU1lZ2fHn4cOHxxlnnBGPP/54x7YhQ4Z0RE5EREVFRWzdurXj66amppgzZ06cfvrpUVpaGiUlJfHyyy/v9zkBSIMzOgD0ixkzZsSYMWNiyZIlceKJJ0Z7e3uMHz8+9u7de9DHPProozt9nZeXF1mWdXw9e/bseOGFF+Kb3/xmjBkzJoqKiqKysvKQnhOAI4PQAaDPvfDCC/HEE0/EkiVL4rzzzouIiIceeuiA+/3617+Ov/u7v4uIiBdffDH++Mc/xllnndXj53344YfjW9/6Vlx88cUREbF58+bYvn37QbwCAI40QgeAPnfcccfF8ccfH9/5zneioqIiNm3aFDfccMMB95s/f34cf/zxUVZWFjfeeGOMGDEiLrvssh4/7+mnnx7Lli2LyZMnR0tLS3z2s5+NY4455hBeCQBHCp/RAaDP5efnx/Lly2PNmjUxfvz4uO666+K222474H633nprXHvttTFp0qRobGyMn/70p1FYWNjj573rrrvixRdfjHPPPTcuv/zy+Nd//dcYOXLkobwUAI4QedlfX8wMAIeBlStXxoUXXhgvvvhiDBs2bKCnA8ARyBkdAAAgOUIHAABIjkvXAACA5DijAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACTn/weCIw1hHWJS3AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgcSFmaEEQR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "2Q4aq1XLEUAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "\n",
        "# Print the report\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FZbOv39ET9J",
        "outputId": "58704ec6-af45-42b7-97e9-2e2505b1dd9c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        19\n",
            "  versicolor       1.00      1.00      1.00        13\n",
            "   virginica       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn\n",
        "\n",
        "!pip install seaborn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "H01-Em_PET7H",
        "outputId": "9f8e6485-62c1-4c67-9fb5-e814410a8669"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(tree, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Score: {best_score}\")\n",
        "\n",
        "# Train a new model with the best parameters\n",
        "best_tree = DecisionTreeClassifier(**best_params, random_state=42)\n",
        "best_tree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_tree.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy on Test Set: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtdpW4B6ET40",
        "outputId": "f0fd3127-b48a-433f-9705-8a15c21689d9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 6}\n",
            "Best Score: 0.9428571428571428\n",
            "Accuracy on Test Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNuOCxthET20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eU_JN02_ET0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ljrMAA24ETyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t6UbPD6JETwm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}